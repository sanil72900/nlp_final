{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"../10_cleaned_data/processed_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(stop_words='english', lowercase=False, ngram_range = (1,1), tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, dataset['label'], test_size = 0.25, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score with bag of words multinomial: 86.84%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "predicted = MNB.predict(X_test)\n",
    "accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "print('accuracy score with bag of words multinomial: ' +str('{:04.2f}'.format(accuracy_score*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score with tf-idf multinomial: 87.45%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "text_count_2 = tfidf.fit_transform(dataset['text'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(text_count_2, dataset['label'], test_size = 0.25, random_state=5)\n",
    "MNB.fit(x_train, y_train)\n",
    "accuracy_score_mnb = metrics.accuracy_score(MNB.predict(x_test), y_test)\n",
    "print('accuracy score with tf-idf multinomial: ' +str('{:4.2f}'.format(accuracy_score_mnb*100)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNB accuracy with bag of words bernoulli: 86.22%\n",
      "BNB accuracy with tf idf bernoulli:  84.88%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "#with bag of words encoding\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(X_train, Y_train)\n",
    "accuracy_score_bnb = metrics.accuracy_score(BNB.predict(X_test), Y_test)\n",
    "print('BNB accuracy with bag of words bernoulli: ' + str('{:4.2f}'.format(accuracy_score_bnb*100)+'%'))\n",
    "#with tf-idf\n",
    "BNB = BernoulliNB()\n",
    "BNB.fit(x_train, y_train)\n",
    "accuracy_score_bnb = metrics.accuracy_score(BNB.predict(x_test), y_test)\n",
    "print('BNB accuracy with tf idf bernoulli:  ' + str('{:4.2f}'.format(accuracy_score_bnb*100)+'%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(naive_bayes_model, vocabulary):\n",
    "\n",
    "    total_samples_required = len(data)\n",
    "    prior_word_prob = np.exp(naive_bayes_model.feature_log_prob_)\n",
    "\n",
    "    # Generating Positive Samples - class 1\n",
    "    pos_sentences = []\n",
    "    for n in range((total_samples_required // 2)):\n",
    "        word_list = random.choices(\n",
    "            vocabulary, prior_word_prob[1], k=random.randint(5, 15)\n",
    "        )\n",
    "        pos_sentences.append(\" \".join(word_list))\n",
    "    pos_df = pd.DataFrame({\"Title\": pos_sentences, \"sentiment\": 1})\n",
    "\n",
    "    # Generating Negative Samples - class 0\n",
    "    neg_sentences = []\n",
    "    for n in range((total_samples_required // 2)):\n",
    "        word_list = random.choices(\n",
    "            vocabulary, prior_word_prob[0], k=random.randint(5, 15)\n",
    "        )\n",
    "        neg_sentences.append(\" \".join(word_list))\n",
    "    neg_df = pd.DataFrame({\"Title\": neg_sentences, \"sentiment\": 0})\n",
    "    synthetic_data = pd.concat([pos_df, neg_df])\n",
    "    synthetic_data.to_csv(\n",
    "        f'synthetic_data_{dt.now().strftime(\"%m_%d_%Y_%H_%M_%S\")}.csv'\n",
    "    )\n",
    "\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "disregard what comes after we probably don't need this now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 22044)\t0.3148968935422105\n",
      "  (0, 9693)\t0.2825194971254728\n",
      "  (0, 860)\t0.27641665042723856\n",
      "  (0, 4483)\t0.3119469280248466\n",
      "  (0, 9613)\t0.3453605162027167\n",
      "  (0, 14104)\t0.14627218784002882\n",
      "  (0, 8020)\t0.27329191804184777\n",
      "  (0, 11364)\t0.16568354667452698\n",
      "  (0, 25815)\t0.1665806981698989\n",
      "  (0, 16014)\t0.20231825735740416\n",
      "  (0, 12276)\t0.105972495765561\n",
      "  (0, 9327)\t0.17089109784576476\n",
      "  (0, 21607)\t0.18303861108371627\n",
      "  (0, 25097)\t0.11366699560340493\n",
      "  (0, 2291)\t0.2757238537850669\n",
      "  (0, 4762)\t0.10816829035911245\n",
      "  (0, 17727)\t0.09716000188202065\n",
      "  (0, 10729)\t0.19319498884977998\n",
      "  (0, 3383)\t0.11269745729344553\n",
      "  (0, 4595)\t0.11853987444351503\n",
      "  (0, 23837)\t0.09761777115774446\n",
      "  (0, 12818)\t0.0987327159688658\n",
      "  (0, 16324)\t0.10039095157488408\n",
      "  (0, 12763)\t0.2415803911364989\n",
      "  (1, 664)\t0.42900714497315645\n",
      "  :\t:\n",
      "  (21246, 8068)\t0.3505007888400098\n",
      "  (21246, 5710)\t0.3026437514629066\n",
      "  (21246, 11191)\t0.26195474788861206\n",
      "  (21246, 9039)\t0.24708345165092135\n",
      "  (21246, 26101)\t0.278715232774355\n",
      "  (21246, 11579)\t0.20569347870903754\n",
      "  (21246, 2696)\t0.24127225478023534\n",
      "  (21246, 16556)\t0.16912394698537433\n",
      "  (21246, 11364)\t0.1369195768361878\n",
      "  (21246, 23326)\t0.21279485569397927\n",
      "  (21246, 16221)\t0.12528037892819752\n",
      "  (21246, 26315)\t0.11308583255238666\n",
      "  (21246, 7820)\t0.15585601661595747\n",
      "  (21246, 11925)\t0.16936535744642567\n",
      "  (21246, 4792)\t0.12887257923743212\n",
      "  (21246, 21780)\t0.11721422353054532\n",
      "  (21246, 21128)\t0.19767936475185544\n",
      "  (21246, 14961)\t0.13178446671555297\n",
      "  (21246, 26018)\t0.1407439520094748\n",
      "  (21246, 23481)\t0.07462638088104565\n",
      "  (21246, 23601)\t0.08422436876443022\n",
      "  (21246, 15771)\t0.1447176136758203\n",
      "  (21246, 8852)\t0.13995993144031074\n",
      "  (21246, 23837)\t0.08067055653308944\n",
      "  (21246, 12763)\t0.06654671026026832\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27779    1\n",
      "7213     0\n",
      "2186     0\n",
      "20875    1\n",
      "24255    1\n",
      "        ..\n",
      "3046     0\n",
      "26301    1\n",
      "20463    1\n",
      "18638    1\n",
      "2915     0\n",
      "Name: label, Length: 21247, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34134283, 0.65865717],\n",
       "       [0.86395327, 0.13604673],\n",
       "       [0.76390166, 0.23609834],\n",
       "       ...,\n",
       "       [0.21459147, 0.78540853],\n",
       "       [0.52041587, 0.47958413],\n",
       "       [0.94496501, 0.05503499]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first number 0, second number 1\n",
    "MNB.predict_proba(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "z = MNB.predict_proba(x_train)\n",
    "labels = np.argmax(z, axis=1)\n",
    "classes = MNB.classes_\n",
    "labels = [classes[i] for i in labels]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# only need x_train because it derives y value using argmax on z\n",
    "def combine(x_train):\n",
    "   # empty dictionary\n",
    "    combo = []\n",
    "    \n",
    "    z = MNB.predict_proba(x_train)\n",
    "    keys = np.argmax(z, axis=1)\n",
    "    values = np.amax(z, axis=1)\n",
    "    \n",
    "    # fill list with keys and values\n",
    "    for i in range(len(keys)):\n",
    "        combo.append([keys[i], values[i]])\n",
    "\n",
    "    print(combo)\n",
    "    \n",
    "    return combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(x_train):\n",
    "    # empty dictionary\n",
    "    synthetic = []\n",
    "\n",
    "    # get combined list of labels and percentages\n",
    "    combo = combine(x_train)\n",
    "\n",
    "    for range(len(combo)):\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call to get synthetic data\n",
    "synthetic = synthetic_data(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create a dictionary with keys as labels from y vector (0,1) and values are multinomial tf-idf predicted probabilities\n",
    "\n",
    "def create_synthetic_data(X, y, model, num_samples):\n",
    "    predicted_prob = {}\n",
    "    for i in range(2):\n",
    "        predicted_prob[i] = model.predict_proba(X[y==i])[:,1]\n",
    "    # create synthetic data\n",
    "    synthetic_data = []\n",
    "    synthetic_labels = []\n",
    "    for i in range(2):\n",
    "        for j in range(num_samples):\n",
    "            # select a random label from the original data\n",
    "            random_label = np.random.choice(y.unique())\n",
    "            # select a random index from the original data\n",
    "            random_index = np.random.choice(len(predicted_prob[random_label]))\n",
    "            # select a random probability from the predicted probabilities of the selected label\n",
    "            random_prob = np.random.choice(predicted_prob[random_label])\n",
    "            # if the random probability is greater than the predicted probability of the original data, then flip the label\n",
    "            if random_prob > predicted_prob[i][random_index]:\n",
    "                synthetic_labels.append(1-i)\n",
    "            else:\n",
    "                synthetic_labels.append(i)\n",
    "            # append the original data to the synthetic data\n",
    "            synthetic_data.append(X[y==i].iloc[random_index])\n",
    "    return synthetic_data, synthetic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1268594570.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [53], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    xs_train, xs_test, ys_train, ys_test = train_test_split(******, dataset['label'], test_size = 0.25, random_state=5)\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# test the synthetic data with the original model\n",
    "xs_train, xs_test, ys_train, ys_test = train_test_split(******, dataset['label'], test_size = 0.25, random_state=5)\n",
    "MNB.fit(xs_train, ys_train)\n",
    "accuracy_score_mnb = metrics.accuracy_score(MNB.predict(xs_test), ys_test)\n",
    "print('accuracy score with synthetic data and tf-idf multinomial: ' +str('{:4.2f}'.format(accuracy_score_mnb*100)) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8fd83cd15327c605b5d2b27007ab6e0fb7a342e364f0dfb271a837713f96538"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
